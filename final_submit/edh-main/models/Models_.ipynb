{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модели для бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automl aproaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T16:53:53.938952Z",
     "iopub.status.busy": "2021-05-29T16:53:53.938505Z",
     "iopub.status.idle": "2021-05-29T16:53:54.030714Z",
     "shell.execute_reply": "2021-05-29T16:53:54.029701Z",
     "shell.execute_reply.started": "2021-05-29T16:53:53.938834Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h2o'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3e144f4284f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeeplearning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mH2OAutoEncoderEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'h2o'"
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "from h2o.estimators.deeplearning import H2OAutoEncoderEstimator\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = h2o.import_file(PATH + \"/train.csv\")\n",
    "testg = h2o.import_file(PATH + \"/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = H2OAutoEncoderEstimator( \n",
    "        activation=\"Tanh\", \n",
    "        hidden=[50], \n",
    "        l1=1e-5,\n",
    "        score_interval=0,\n",
    "        epochs=100\n",
    ")\n",
    "\n",
    "model.train(x=train_ecg.names, training_frame=train_ecg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_error = model.anomaly(test_ecg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reconstruction_error.as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rank'] = df['Reconstruction.MSE'].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values('Rank')\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = df_sorted[ df_sorted['Reconstruction.MSE'] > 1.0 ]\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель для rare events на деревьях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.discrete.discrete_model import Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedClassifier(object):\n",
    "    ''' A set of tools to help with classification problems for imbalanced data.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        X_train = self.X_train\n",
    "        X_test = self.X_test\n",
    "        y_train = self.y_train\n",
    "        y_test = self.y_test\n",
    "        dep_variable=self.dep_variable\n",
    "\n",
    "    def sample_abundant_data(self, tolerance=0.20):\n",
    "        '''Create a sample from the abundant class of a binary dependent variable.\n",
    "        INPUTS:\n",
    "        df (dataframe) - A pandas dataframe containing the set of features for modeling.\n",
    "        y_df (dataframe) - A pandas dataframe containing the dependent variable for which to produce the sample.\n",
    "        dep_variable (str) - The dataframe column representing the dependent variable, stored as 0/1 boolean values.\n",
    "        tolerance (float) - A tolerance factor for the number of samples to produce.  The resulting sample will be\n",
    "        between 1 +/- tolerance of the rare events.\n",
    "        RETURNS:\n",
    "        X_tr (dataframe) - A new dataframe containing all instances where dep_variable == 1 and the sampled rows\n",
    "        where dep_variable == 0.\n",
    "        y_tr (Pandas data series) - A new Pandas data series of the response variable based on the sample.\n",
    "        '''\n",
    "        df = pd.merge(self.X_train, pd.DataFrame(self.y_train, columns=[self.dep_variable], index=self.y_train.index),\\\n",
    "        how='inner', left_index=True, right_index=True)\n",
    "        y_1 = df.loc[df[self.dep_variable] == 1]\n",
    "\n",
    "        sample_pct = random.uniform(1 - tolerance, 1 + tolerance)\n",
    "        sample_size = int(np.sum(df[self.dep_variable]) * sample_pct)\n",
    "\n",
    "        samp = df.loc[df[self.dep_variable] == 0].sample(n=sample_size)\n",
    "\n",
    "        new_x = pd.concat([samp, y_1], axis=0)\n",
    "        new_y = new_x.pop(dep_variable)\n",
    "        return new_x, new_y\n",
    "\n",
    "    def bootstrap_sample(self, tolerance=0.20):\n",
    "        '''Create bootstrap samples from the majority and minority class of a data frame.  The resulting\n",
    "        samples are used in balanced random forests (http://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf)\n",
    "        and gradient boosting algorithms.\n",
    "        INPUTS:\n",
    "        df (dataframe) - A pandas dataframe containing the set of features for modeling.\n",
    "        y_df (dataframe) - A pandas dataframe containing the dependent variable for which to produce the sample.\n",
    "        dep_variable (str) - The dataframe column representing the dependent variable, stored as 0/1 boolean values.\n",
    "        tolerance (float) - A tolerance factor for the number of samples to produce.  The resulting sample will be\n",
    "        between 1 +/- tolerance of the rare events.\n",
    "        RETURNS:\n",
    "        X_tr (dataframe) - A new dataframe containing all instances where dep_variable == 1 and the sampled rows\n",
    "        where dep_variable == 0.\n",
    "        y_tr (Pandas data series) - A new Pandas data series of the response variable based on the sample.\n",
    "        '''\n",
    "        df = pd.merge(self.X_train, pd.DataFrame(self.y_train, columns=[self.dep_variable], index=self.y_train.index),\\\n",
    "        how='inner', left_index=True, right_index=True)\n",
    "\n",
    "        sample_pct = random.uniform(1 - tolerance, 1 + tolerance)\n",
    "        sample_size = int(np.sum(df[self.dep_variable]) * sample_pct)\n",
    "\n",
    "        samp0 = df.loc[df[self.dep_variable] == 0].sample(n=sample_size, replace=True)\n",
    "        samp1 = df.loc[df[self.dep_variable] == 1].sample(n=sample_size, replace=True)\n",
    "\n",
    "        new_x = pd.concat([samp0, samp1], axis=0)\n",
    "        new_y = new_x.pop(dep_variable)\n",
    "        return new_x, new_y\n",
    "\n",
    "    def get_combined_proba(models, X_train, X_test, y_train, y_test, sample_method='abundant', ksamples=15):\n",
    "        '''sample_method = 'abundant' or 'bootstrap'\n",
    "        '''\n",
    "        sample_methods={'abundant': sample_abundant_data, 'bootstrap': bootstrap_sample}\n",
    "        predictions = []\n",
    "        for k in range(ksamples):\n",
    "            Xt, Yt = sample_methods[sample_method](X_train, y_train, dep_variable='N188')\n",
    "            for model in models.values():\n",
    "                model.fit(Xt, Yt)\n",
    "                p = model.predict_proba(X_test)\n",
    "                predictions.append(p)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def get_majority_vote(models, modname='Default Model',sample_method='abundant', ksamples=15, print_results=True):\n",
    "        ''' Fit k models using either abundant of bootstrap samples and create classification predictions.\n",
    "        Use a majority vote of the k samples to produce a final classification prediction.\n",
    "        INPUTS:\n",
    "        models (dict) - A dictionary of model class instantiation references.\n",
    "        E.g. if gb=GradientBoostingClassifier, lr=LogisticRegression, then models = {'gb': gb, 'lr': lr}\n",
    "        modname (str) - A string for printing the algorithm name if quality metrics are printed.\n",
    "        sample_method (str) - 'abundant' or 'bootstrap', the preferred sample method to use.\n",
    "        k_samples (int) - The number of samples to take from the training dataset\n",
    "        print_results (bool) - A boolean of whether or not quality metrics should be printed for final results.\n",
    "        RETURNS:\n",
    "        roc_auc (float) - The ROC AUC (area under the receiver/operator characteristic)\n",
    "        votes (Numpy Array) - An array of the binary predictions for y_test\n",
    "        probs (Numpy Array) - An array of the probabilities of positive prediction for y_test\n",
    "        '''\n",
    "        sample_methods={'abundant': sample_abundant_data, 'bootstrap': bootstrap_sample}\n",
    "        predictions = []\n",
    "        probs = np.zeros(len(y_test))\n",
    "        for k in range(ksamples):\n",
    "            Xt, Yt = sample_methods[sample_method](self.X_train, self.y_train, dep_variable=self.dep_variable)\n",
    "            for model in models.values():\n",
    "                model.fit(Xt, Yt)\n",
    "                p = model.predict(X_test)\n",
    "                predictions.append(p)\n",
    "                prob = model.predict_proba(self.X_test)[:,1]\n",
    "                probs = np.sum([probs, prob], axis=0)\n",
    "\n",
    "        votes = np.array([1 if sum(x) > (len(predictions) / 2) else 0 for x in zip(*predictions)])\n",
    "        probs = probs / len(predictions)\n",
    "        accuracy = accuracy_score(self.y_test, votes)\n",
    "        precision = precision_score(self.y_test, votes)\n",
    "        cm = confusion_matrix(self.y_test, votes)\n",
    "        recall = recall_score(self.y_test, votes)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        if print_results is True:\n",
    "            model = 'combined'\n",
    "            self.print_quality_metrics(model, modname, self.X_test, self.y_test)\n",
    "\n",
    "        return roc_auc, votes, probs\n",
    "\n",
    "    def print_quality_metrics(model, model_name):\n",
    "        '''Print basic quality metrics for a given model, including:\n",
    "        confusion matrix, AUC, accuracy, precision and recall.\n",
    "        INPUTS:\n",
    "        model - The instantiated class name of the model used (e.g. gb, lr, rf)\n",
    "        model_name (str) - The text description of the model for use in printing.\n",
    "        RETURNS:\n",
    "        None\n",
    "        '''\n",
    "        accuracy = accuracy_score(self.y_test, model.predict(self.X_test))\n",
    "        precision = precision_score(self.y_test, model.predict(self.X_test))\n",
    "        recall = recall_score(self.y_test, model.predict(self.X_test))\n",
    "        fpr, tpr, thresholds = roc_curve(self.y_test, model.predict(self.X_test))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        cm = confusion_matrix(self.y_test, model.predict(self.X_test))\n",
    "        print (\"Model Name: Accuracy\\tPrecision\\tRecall\\tAUC\")\n",
    "        print('{0}: {1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}'.format(model_name, accuracy, precision, recall, roc_auc))\n",
    "        print (\"Confusion Matrix\")\n",
    "        print (cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = 'N189'\n",
    "\n",
    "X_train = \n",
    "X_test = \n",
    "y_train = \n",
    "y_test = \n",
    "\n",
    "gb = GradientBoostingClassifier(learning_rate=0.005, n_estimators=500,\\\n",
    "max_features='sqrt', max_depth=5)\n",
    "\n",
    "models = {'Gradient Boosting': model1}\n",
    "model1 = {'gb': gb}\n",
    "\n",
    "for modname, model in models.items():\n",
    "    roc_auc, votes, probs = get_majority_vote(model, X_train, X_test, y_train, y_test,\\\n",
    "    modname=modname, dep_variable=dep, sample_method='abundant', ksamples=k_samp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель для rare events на автоэнкодере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pylab import rcParams\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from numpy.random import seed\n",
    "SEED = 123 #used to help randomly select the data points\n",
    "DATA_SPLIT_PCT = 0.2\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "LABELS = [\"Normal\",\"Danger\",'Very_Danger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_train_0_x)\n",
    "df_train_0_x_rescaled = scaler.transform(df_train_0_x)\n",
    "df_valid_0_x_rescaled = scaler.transform(df_valid_0_x)\n",
    "df_valid_x_rescaled = scaler.transform(df_valid.drop(['y'], axis = 1))df_test_0_x_rescaled = scaler.transform(df_test_0_x)\n",
    "df_test_x_rescaled = scaler.transform(df_test.drop(['y'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 200\n",
    "batch_size = 128\n",
    "input_dim = df_train_0_x_rescaled.shape[1] #num of predictor variables, \n",
    "encoding_dim = 32\n",
    "hidden_dim = int(encoding_dim / 2)\n",
    "learning_rate = 1e-3\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"relu\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "encoder = Dense(hidden_dim, activation=\"relu\")(encoder)\n",
    "decoder = Dense(hidden_dim, activation=\"relu\")(encoder)\n",
    "decoder = Dense(encoding_dim, activation=\"relu\")(decoder)\n",
    "decoder = Dense(input_dim, activation=\"linear\")(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(metrics=['accuracy'],\n",
    "                    loss='mean_squared_error',\n",
    "                    optimizer='adam')cp = ModelCheckpoint(filepath=\"autoencoder_classifier.h5\",\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)tb = TensorBoard(log_dir='./logs',\n",
    "                histogram_freq=0,\n",
    "                write_graph=True,\n",
    "                write_images=True)history = autoencoder.fit(df_train_0_x_rescaled, df_train_0_x_rescaled,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(df_valid_0_x_rescaled, df_valid_0_x_rescaled),\n",
    "                    verbose=1,\n",
    "                    callbacks=[cp, tb]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_predictions = autoencoder.predict(df_valid_x_rescaled)\n",
    "mse = np.mean(np.power(df_valid_x_rescaled - valid_x_predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                        'True_class': df_valid['y']})precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)\n",
    "plt.plot(threshold_rt, precision_rt[1:], label=\"Precision\",linewidth=5)\n",
    "plt.plot(threshold_rt, recall_rt[1:], label=\"Recall\",linewidth=5)\n",
    "plt.title('Precision and recall for different threshold values')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_predictions = autoencoder.predict(df_test_x_rescaled)\n",
    "mse = np.mean(np.power(df_test_x_rescaled - test_x_predictions, 2), axis=1)\n",
    "error_df_test = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                        'True_class': df_test['y']})\n",
    "error_df_test = error_df_test.reset_index()threshold_fixed = 0.4\n",
    "groups = error_df_test.groupby('True_class')fig, ax = plt.subplots()for name, group in groups:\n",
    "    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "            label= \"Break\" if name == 1 else \"Normal\")\n",
    "ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.legend()\n",
    "plt.title(\"Reconstruction error for different classes\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]conf_matrix = confusion_matrix(error_df.True_class, pred_y)plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель для анализа выживаемости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
